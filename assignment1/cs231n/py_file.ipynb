{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from past.builtins import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)       \n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "    N, D = X.shape\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    y = np.eye(C)[y] # one_hot_y shape N * C\n",
    "    XW = np.dot(X,W) #shape X : N*D, #shape W : D*C  #XW shape : N * C\n",
    "    softmax_XW = (np.exp(XW)) / (np.sum(np.exp(XW), axis = 1).reshape(-1,1)) #shape : N*C\n",
    "    \n",
    "    loss = -np.log(softmax_XW[np.arange(len(softmax_XW)), #Loss\n",
    "                              np.argmax(y, axis = 1)]).sum()\n",
    "    loss /= N\n",
    "    loss += (reg * (W**2).sum()) / 2\n",
    "    \n",
    "    dW /= N \n",
    "    dW += reg*W\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_vectorized(W, X, y, reg):\n",
    "    loss = 0.0\n",
    "    dW = np.zeros_like(W)\n",
    "    \n",
    "    N, D = X.shape\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    y = np.eye(C)[y] # one_hot_y shape N * C\n",
    "    XW = np.dot(X,W) #shape X : N*D, #shape W : D*C  #XW shape : N * C\n",
    "    softmax_XW = (np.exp(XW)) / (np.sum(np.exp(XW), axis = 1).reshape(-1,1)) #shape : N*C\n",
    "    \n",
    "    loss = -np.log(softmax_XW[np.arange(len(softmax_XW)), #Loss\n",
    "                              np.argmax(y, axis = 1)]).sum()\n",
    "    loss /= N\n",
    "    loss += (reg * (W**2).sum()) / 2\n",
    "    \n",
    "    \n",
    "    softmax_XW[np.arange(len(softmax_XW)), np.argmax(y, axis=1)] -= 1\n",
    "    \n",
    "    dW = X.T.dot(softmax_XW) / N\n",
    "    dW += reg * W\n",
    "    \n",
    "    return loss, dW\n",
    "  # Initialize the loss and gradient to zero.\n",
    "    \n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
    "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
    "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
    "  # regularization!                                                           #\n",
    "  #############################################################################\n",
    "\n",
    "  #############################################################################\n",
    "  #                          END OF YOUR CODE                                 #\n",
    "  #############################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
